{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pRA6jDYDoPJ"
   },
   "source": [
    "# ___Implementation of CNN with Keras using TensorFlow backend___"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "id": "Loo3nkxLDoPK"
   },
   "source": [
    "## ___Dog Cat Classifier___\n",
    "\n",
    "_To explore and showcase how this technique can be used, I conducted a small experiment using dataset provided [on this page](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data). The train folder contains 25,000 images of dogs and cats._\n",
    "\n",
    "_For this exercise it’s advisable to arrange the the folders that contain images as shown below. We separate images into folders and give them their appropriate names, i.e the training set and the test set. This makes it easier to import the images into Keras. Make sure that the working directory has permissions to access the images._\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbEZumPQDoPL"
   },
   "source": [
    "### ___Setup___\n",
    "_In this step we need to import Keras and other packages that we’re going to use in building the CNN. Import the following packages:_\n",
    "\n",
    "* ___Sequential___ _is used to initialize the neural network._\n",
    "* ___Convolution2D___ _is used to make the convolutional network that deals with the images._\n",
    "* ___MaxPooling2D___ _layer is used to add the pooling layers._\n",
    "* ___Flatten___ _is the function that converts the pooled feature map to a single column that is passed to the fully connected layer._\n",
    "* ___Dense___ _adds the fully connected layer to the neural network._\n",
    "* ___Reshape___ _is used to change the shape of the input array_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "id": "Nrk1L3r6DoPM"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Edf3XDu5DoPT"
   },
   "source": [
    "### ___Initializing the Neural Network___\n",
    "_To initialize the neural network we create an object of the Sequential class._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FosBUEnVDoPU"
   },
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LN8hNAfCDoPY"
   },
   "source": [
    "### ___Convolution___\n",
    "\n",
    "_To add the convolution layer, we call the add function with the classifier object and pass in Convolution2D with parameters. The first argument nb_filter. nbfilter is the number of feature detectors that we want to create. The second and third parameters are dimensions of the feature detector matrix._\n",
    "\n",
    "_It’s common practice to start with 32 feature detectors for CNNs. The next parameter is input_shape which is the shape of the input image. The images will be converted into this shape during preprocessing. If the image is black and white it will be converted into a 2D array and if the image is colored it will be converted into a 3D array._\n",
    "\n",
    "_In this case, we’ll assume that we are working with colored images. Input_shape is passed in a tuple with the number of channels, which is 3 for a colored image, and the dimensions of the 2D array in each channel. If you are not using a GPU it’s advisable to use lower dimensions to reduce the computation time. When using a CPU, 64 by 64 dimensions performs well. The final parameter is the activation function. Classifying images is a nonlinear problem. So we use the rectifier function to ensure that we don’t have negative pixel values during computation. That’s how we achieve non-linearity._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vQSGujhQDoPZ"
   },
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(32, (3, 3), input_shape = (256, 256, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9V-70iSNDoPd"
   },
   "source": [
    "### ___Pooling___\n",
    "_In this step we reduce the size of the feature map. Generally we create a pool size of 2x2 for max pooling. This enables us to reduce the size of the feature map while not losing important image information._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XffZOP_bDoPe"
   },
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyPElriJDoPi"
   },
   "source": [
    "### ___Flattening___\n",
    "_In this step, all the pooled feature maps are taken and put into a single vector. The Flatten function flattens all the feature maps into a single column._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rXhDE71TDoPj"
   },
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJQN0qA0DoPo"
   },
   "source": [
    "### ___Full Connection___\n",
    "_The next step is to use the vector we obtained above as the input for the neural network by using the Dense function in Keras. The first parameter is output_dim which is the number of nodes in the hidden layer. You can determine the most appropriate number through experimentation. The higher the number of dimensions the more computing resources you will need to fit the model. A common practice is to pick the number of nodes in powers of two. The second parameter is the activation function. We usually use the ReLu activation function in the hidden layer._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IvOGDsaXDoPp"
   },
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6FoG6duDoPt"
   },
   "source": [
    "_The next layer we have to add is the output layer. In this case, we’ll use the sigmoid activation function since we expect a binary outcome. If we expected more than two outcomes we would use the softmax function._\n",
    "\n",
    "_The output_dim here is 1 since we just expect the predicted probabilities of the classes._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "40DnypUiDoPu"
   },
   "outputs": [],
   "source": [
    "classifier.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wEhOnA7iDoPx"
   },
   "source": [
    "### ___Compiling CNN___\n",
    "_We then compile the CNN using the compile function. This function expects three parameters: the optimizer, the loss function, and the metrics of performance.The optimizer is the gradient descent algorithm we are going to use. We use the `binary_crossentropy` loss function since we are doing a binary classification._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hA8rILiRDoPy"
   },
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6zxr7Y1DoP2"
   },
   "source": [
    "### ___Fitting the CNN___\n",
    "_We are going to preprocess the images using Keras to prevent overfitting. This processing is known as image augmentation. The Keras utility we use for this purpose is `ImageDataGenerator`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "B3vi0l4eDoP2"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ug1U_hVnDoP6"
   },
   "source": [
    "_This function works by __flipping, rescaling, zooming, and shearing__ the images. The first argument rescale ensures the images are rescaled to have pixel values between zero and one. `horizontal_flip=True` means that the images will be flipped horizontally. All these actions are part of the image augmentation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KPl1LpgHDoP8"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   shear_range=0.2, \n",
    "                                   zoom_range=0.2, \n",
    "                                   horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEhIM7KjDoQA"
   },
   "source": [
    "_We then use the `ImageDataGenerator` function to rescale the pixels of the test set so that they are between zero and one. Since this is the test data and not the training data we don’t have to take image augmentation steps._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nYbXMPo5DoQB"
   },
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z518lEogDoQF"
   },
   "source": [
    "_The next thing we need to do is create the training set. We do this by using `train_datagen` that we just created above and the `flow_from_directory` function. The `flow_from_directory` function enables us to retrieve the images of our training set from the current working directory. The first parameter is the path to the training set._\n",
    "\n",
    "_The second parameter is the `target_size`, which is the size of the image that the CNN should expect. We have already specified this above as 256x256, so we shall use the same for this parameter. The `batch_size` is the number of images that will go through the network before the weights are updated. The `class_mode` parameter indicates whether the classification is binary or not._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "zoqHNf5pDoQF",
    "outputId": "4d411ef4-61eb-48c3-9150-946af8a21840"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2050 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('/content/training_set', \n",
    "                                                 target_size=(256, 256), \n",
    "                                                 batch_size=32, \n",
    "                                                 class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGeVePDkDoQJ"
   },
   "source": [
    "_Now we will create the test set with similar parameters as above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "MTNiLtdKDoQK",
    "outputId": "470af8ec-3feb-4afe-b3ce-8553366de363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory('/content/test_set', \n",
    "                                            target_size=(256, 256), \n",
    "                                            batch_size=32, \n",
    "                                            class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzBHJ3tvDoQO"
   },
   "source": [
    "_Finally, we need to fit the model to the training dataset and test its performance with the test set. We achieve this by calling the `fit_generator` function on the classifier object. The first argument it takes is the training set. The second argument is the number of arguments in our training set. `Epochs` is the number of epochs we want to use to train the CNN. Validation_data is the test data set. `nb_val_samples` is the number of images in the test set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 973
    },
    "id": "iUG2WtUHDoQO",
    "outputId": "a4c59fc2-9a5e-44ed-ee36-032d53e861a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-e26b0a538fac>:5: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/25\n",
      "40/40 [==============================] - ETA: 0s - loss: -12520.5469 - accuracy: 0.5400WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1000 batches). You may need to use the repeat() function when building your dataset.\n",
      "40/40 [==============================] - 20s 495ms/step - loss: -12520.5469 - accuracy: 0.5400 - val_loss: -54668.2969 - val_accuracy: 0.5000\n",
      "Epoch 2/25\n",
      "40/40 [==============================] - 19s 477ms/step - loss: -174502.3281 - accuracy: 0.5792\n",
      "Epoch 3/25\n",
      "40/40 [==============================] - 19s 484ms/step - loss: -878882.3125 - accuracy: 0.5695\n",
      "Epoch 4/25\n",
      "40/40 [==============================] - 19s 465ms/step - loss: -2653763.7500 - accuracy: 0.5712\n",
      "Epoch 5/25\n",
      "40/40 [==============================] - 19s 467ms/step - loss: -6491837.0000 - accuracy: 0.5480\n",
      "Epoch 6/25\n",
      "40/40 [==============================] - 19s 466ms/step - loss: -11855702.0000 - accuracy: 0.5760\n",
      "Epoch 7/25\n",
      "40/40 [==============================] - 19s 464ms/step - loss: -20621722.0000 - accuracy: 0.5624\n",
      "Epoch 8/25\n",
      "40/40 [==============================] - 19s 463ms/step - loss: -33090712.0000 - accuracy: 0.5696\n",
      "Epoch 9/25\n",
      "40/40 [==============================] - 18s 456ms/step - loss: -49297732.0000 - accuracy: 0.5632\n",
      "Epoch 10/25\n",
      "40/40 [==============================] - 19s 468ms/step - loss: -70469336.0000 - accuracy: 0.5664\n",
      "Epoch 11/25\n",
      "40/40 [==============================] - 19s 472ms/step - loss: -97903944.0000 - accuracy: 0.5617\n",
      "Epoch 12/25\n",
      "40/40 [==============================] - 18s 457ms/step - loss: -125891224.0000 - accuracy: 0.5776\n",
      "Epoch 13/25\n",
      "40/40 [==============================] - 18s 460ms/step - loss: -171545360.0000 - accuracy: 0.5624\n",
      "Epoch 14/25\n",
      "40/40 [==============================] - 19s 463ms/step - loss: -216877184.0000 - accuracy: 0.5594\n",
      "Epoch 15/25\n",
      "40/40 [==============================] - 18s 458ms/step - loss: -273078080.0000 - accuracy: 0.5568\n",
      "Epoch 16/25\n",
      "40/40 [==============================] - 18s 456ms/step - loss: -333084832.0000 - accuracy: 0.5568\n",
      "Epoch 17/25\n",
      "40/40 [==============================] - 18s 456ms/step - loss: -408717216.0000 - accuracy: 0.5560\n",
      "Epoch 18/25\n",
      "40/40 [==============================] - 19s 470ms/step - loss: -468596384.0000 - accuracy: 0.5750\n",
      "Epoch 19/25\n",
      "40/40 [==============================] - 18s 460ms/step - loss: -570743808.0000 - accuracy: 0.5576\n",
      "Epoch 20/25\n",
      "40/40 [==============================] - 19s 463ms/step - loss: -665035264.0000 - accuracy: 0.5576\n",
      "Epoch 21/25\n",
      "40/40 [==============================] - 18s 454ms/step - loss: -768029248.0000 - accuracy: 0.5608\n",
      "Epoch 22/25\n",
      "40/40 [==============================] - 18s 451ms/step - loss: -892544320.0000 - accuracy: 0.5528\n",
      "Epoch 23/25\n",
      "40/40 [==============================] - 19s 466ms/step - loss: -1001709248.0000 - accuracy: 0.5641\n",
      "Epoch 24/25\n",
      "40/40 [==============================] - 19s 466ms/step - loss: -1131952128.0000 - accuracy: 0.5664\n",
      "Epoch 25/25\n",
      "40/40 [==============================] - 18s 459ms/step - loss: -1295397376.0000 - accuracy: 0.5600\n"
     ]
    }
   ],
   "source": [
    "model = classifier.fit_generator(training_set, \n",
    "                                 steps_per_epoch=40, \n",
    "                                 epochs=25, \n",
    "                                 validation_data=test_set, \n",
    "                                 validation_steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xK_gB7l5DoQS"
   },
   "source": [
    "### ___Making a Single Prediction___\n",
    "_Now that the model is fitted, we can use the predict method to make predictions using new images. In order to do this we need to preprocess our images before we pass them to the predict method. To achieve this we’ll use some functions from numpy. We also need to import the image module from Keras to allow us to load in the new images._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "cbaLU2ToDoQT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imjyPyRqDoQY"
   },
   "source": [
    "_The next step is to load the image that we would like to predict. To accomplish this we use the `load_img` function from the image module. The first argument this function takes is the path to the location of the image and the second argument is the size of the image. The size of the image should be the same as the size used during the training process._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "bSkWv-zjDoQY"
   },
   "outputs": [],
   "source": [
    "test_image = image.load_img('/content/test_set/cats/cat.4905.jpg', target_size=(256, 256))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUSnrORiDoQb"
   },
   "source": [
    "_Now we use the predict method to predict which class the image belongs to._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "QCkSyVCsDoQc"
   },
   "outputs": [],
   "source": [
    "prediction = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHMQytycDoQf"
   },
   "source": [
    "_After running this function we’ll get the result: either one or zero. However we don’t know which value represents which class. To find out, we use the class_indices attribute of the training set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "nk30NwLWDoQf",
    "outputId": "21def382-9b92-4d16-ad8f-aa6ac64308a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.ipynb_checkpoints': 0, 'cats': 1, 'dogs': 2}"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "20Xdcq9PLA9Q",
    "outputId": "edccd50b-a33d-442c-9d7d-f263d011a0fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "bLPCJM4pDoQi",
    "outputId": "3ce10b1d-4f4c-4485-e063-8d46984f5d37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "if prediction[0][0] == 2:\n",
    "    value = 'dog'\n",
    "    print(value)\n",
    "else:\n",
    "    value = 'cat'\n",
    "    print(value)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "12. CNN Classifier - Dog Cat Classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
