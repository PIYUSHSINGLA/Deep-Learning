{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___Encoder-Decoder Sequence to Sequence___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___What is Seq2Seq?___\n",
    "\n",
    "_The Seq2Seq(Sequence to Sequence) is a method that can generate another sequence by a specific method based on a given sequence (long sentences, paragraphs, image extraction features, audio signals,etc.). It was firstly proposed in 2014, having first, the two articles describes its main idea, namely Google Brain team \"[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)\" and Yoshua Bengio team \"Learning Phrase Representation using RNN Encoder-Decoder for Statistical Machine Translation. The two articles coincided with a similar solution,and Seq2Seq was generated._\n",
    "\n",
    "_As a simple example, when we use machine translation: input (Hello) ---> output (hello). For another example, in human-machine dialogue, we ask the machine: \"Who are you?\", And the machine will return the answer \"I am XX\"._\n",
    "\n",
    "_The core idea of the seq2Seq model is to convert a sequence signal as an input to an output sequence signal through a deep neural network. This process consists of two processes: __Encoding and Decoding__. In the classic implementation, the encoder and decoder are each __composed of a recurrent neural network(RNN, LSTM, GRU can be)__.In Seq2Seq, the two recurrent neural networks are trained together._\n",
    "\n",
    "<img src='https://www.guru99.com/images/1/111318_0848_seq2seqSequ4.png'/>\n",
    "\n",
    "### ___Applications of Seq2Seq___\n",
    "_With the development of computer technology, artificial intelligence technology, algorithm research, etc. and the needs of social development, Seq2Seq has produced some applications in many fields._\n",
    "\n",
    "* _Machine Translation (currently the most famous Google translation is completely developed based on Seq2Seq + Attention Mechanism)_\n",
    "* _Chatbot(Microsoft Xiaobing, also used seq2seq technology)_\n",
    "* _The text summary is automatically generated(this technology is used by headlines today)_\n",
    "* _The picture discription is automatically genreated_\n",
    "* _Machine writing poetry, code completion, generation of comit message, story style rewriting, etc._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___What is Encoder-Decoder?___\n",
    "\n",
    "_The Encoder-Decoder model is mainly a concept in the NLP field. It does not specifically value a specific algorithm, but a general term for a class of algorithms. Encoder-Decoder can be regarded as a general framework, under which different algorithms can be used to solve different tasks._\n",
    "\n",
    "<img src='https://miro.medium.com/proxy/1*3lj8AGqfwEE5KCTJ-dXTvg.png' width = 400/>\n",
    "\n",
    "_The model consists of 3 parts: __encoder__, __intermediate (encoder) vector__ and __decoder__._\n",
    "\n",
    "<img src='https://miro.medium.com/max/3972/1*1JcHGUU7rFgtXC_mydUA_Q.jpeg' width = 600/>\n",
    "\n",
    "___Encoder___\n",
    "* _A stack of several recurrent units (LSTM or GRU cells for better performance) where each accepts a single element of the input sequence, collects information for that element and propagates it forward._\n",
    "* _In question-answering problem, the input sequence is a collection of all words from the question. Each word is represented as x_i where i is the order of that word._\n",
    "* _The hidden states h_i are computed using the formula:_\n",
    "<img src='https://miro.medium.com/max/700/1*sKqGIDJm3P8DeSwl0WHGkg.png'/>\n",
    "\n",
    "* _This simple formula represents the result of an ordinary recurrent neural network. As you can see, we just apply the appropriate weights to the previous hidden state h_(t-1) and the input vector x_t._\n",
    "\n",
    "* _Example: Consider the input sequence ‚ÄúI am a Student‚Äù to be encoded. There will be totally 4 timesteps ( 4 tokens) for the Encoder model. At each time step, the hidden state h will be updated using the previous hidden state and the current input._\n",
    "\n",
    "* _At the first timestep t1, the previous hidden state h0 will be considered as zero or randomly chosen. So the first RNN cell will update the current hidden state with the first input and h0. Each layer outputs two things ‚Äî updated hidden state and the output for each stage. The outputs at each stage are rejected and only the hidden states will be propagated to the next layer._\n",
    "\n",
    "* _At second timestep t2, the hidden state h1 and the second input X[2] will be given as input , and the hidden state h2 will be updated according to both inputs. Then the hidden state h1 will be updated with the new input and will produce the hidden state h2. This happens for all the four stages wrt example taken._\n",
    "\n",
    "<img src='https://miro.medium.com/max/700/1*J0tt1Xncos1kficN80DjuQ.png' width = 400/>\n",
    "\n",
    "___Encoder Vector___\n",
    "* _This is the final hidden state produced from the encoder part of the model. It is calculated using the formula above._\n",
    "* _This vector aims to encapsulate the information for all input elements in order to help the decoder make accurate predictions._\n",
    "* _It acts as the initial hidden state of the decoder part of the model._\n",
    "\n",
    "___Decoder___\n",
    "* _A stack of several recurrent units where each predicts an output y_t at a time step t._\n",
    "* _Each recurrent unit accepts a hidden state from the previous unit and produces and output as well as its own hidden state._\n",
    "* _In the question-answering problem, the output sequence is a collection of all words from the answer. Each word is represented as y_i where i is the order of that word._\n",
    "* _Any hidden state h_i is computed using the formula:_\n",
    "<img src='https://miro.medium.com/max/700/1*sdxvcjeV7NOUsR_VQ_nrUQ.png'/>\n",
    "* _As you can see, we are just using the previous hidden state to compute the next one._\n",
    "* _The output y_t at time step t is computed using the formula:_\n",
    "\n",
    "<img src='https://miro.medium.com/max/700/1*y5T2-J2mrCRZp5M9Q4METw.png'/>\n",
    "\n",
    "<img src='https://miro.medium.com/max/700/1*cKAWEO4GJ6crmmCkNvNEPQ.png' width = 400/>\n",
    "\n",
    "_We calculate the outputs using the hidden state at the current time step together with the respective weight W(S). Softmax is used to create a probability vector which will help us determine the final output (e.g. word in the question-answering problem)._\n",
    "\n",
    "_The power of this model lies in the fact that it can map sequences of different lengths to each other. As you can see the inputs and outputs are not correlated and their lengths can differ. This opens a whole new range of problems which can now be solved using such architecture._\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/600/1*8OZYn5yMAl4hfdfTwSwKyQ.png'/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Core Idea of the Seq2Seq Model___\n",
    "\n",
    "<img src='https://www.guru99.com/images/1/111318_0848_seq2seqSequ2.png'/>\n",
    "\n",
    "_The Seq2Seq model is mainly used to achieve the conversion from one sequence to another, such as French-English translation. The Seq2Seq model consists of two deep neural networks.The deep neural network can be other neural networks such as RNN(Recurrent neural network) or LSTM(Long short-term memory). The Seq2Seq model uses a neural network to map the input sequence to a fixed-dimensional vector, which is an encoding process; then another neural network maps this vector to the target sequence, which is a decoding process.The model structure of Seq2Seq is shown in Fig1. The model inputs the sentence \"ABC\" and then generates \"WXYZ\" as the output sentence._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ___Seq2Seq Model___\n",
    "\n",
    "<img src='https://docs.chainer.org/en/stable/_images/seq2seq.png' width = 600/>\n",
    "\n",
    "* ___Encoder Embedding Layer___\n",
    "\n",
    "    _The first layer or encoder embedding layer converts each word in input sentence to the embedding vector. When processing the i-th word in the input sentence, the input and output of the layer are the following:_\n",
    "\n",
    "    _The input is  ùë•ùëñ : the one-hot vector which represents the i-th word._\n",
    "\n",
    "    _The output is  ùë•¬Øùëñ : the embedding vector which represents the i-th word._\n",
    "    \n",
    "\n",
    "* ___Encoder Recurrent Layer___\n",
    "\n",
    "    _The encoder recurrent layer generates a hidden vectors from the embedding vectors. When we processing the i-th embedding vector, the input and output layer are the following:_\n",
    "\n",
    "    _The input is  ùë•¬Øùëñ : the embedding vector which represents the i-th word._\n",
    "\n",
    "    _The output vector is  ‚Ñé(ùë†)ùëñ  : hidden vector of the i-th position_\n",
    "\n",
    "  \n",
    "  \n",
    "* ___Decoder Embedding Layer___\n",
    "\n",
    "    _The decoder embedding layer converts each word in the output sentence to the embedding vector. When processing the j-th word in the output sentence, the input and output layer are the following:_\n",
    "\n",
    "    _The input is  ùë¶ùëó‚àí1 : the one-hot vector which represents the (j - 1)-th word generated by the decoder output layer._\n",
    "    \n",
    "    _The output is  ùë¶¬Øùëó : the embedding vector which represents the (j - 1)-th word._\n",
    "    \n",
    "    \n",
    "    \n",
    "* ___Decoder Recurrent layer___\n",
    "\n",
    "    _The decoder recurrent layer generates the hidden vectors from the embedding vectors. When processing the j-th embedding vector, the input and the output layers are following:_\n",
    "    \n",
    "    _The input  ùë¶¬Øùëó  : the embedding vector._\n",
    "    \n",
    "    _The output is  ‚Ñé(ùë°)ùëó  : the hidden vector of j-th position._\n",
    "    \n",
    "    _For example, when using the uni-directional RNN of one layer, the process can be represented as the following function  Œ®(ùë°):_\n",
    "    \n",
    "    _In this case we used tanh as the activation function.And we must use the encoder's hidden vector of the last position as the decoder's hidden vector of first position as following:_\n",
    "    \n",
    "    $$‚Ñé(ùë°)0=ùëß=‚Ñé(ùë†)ùë°$$\n",
    "\n",
    "\n",
    "* ___Decoder Output Layer___\n",
    "\n",
    "    _The decoder output layer generates the probability of the j-th word of the output sentence from the hidden vector. When we processing the j-th embedding vector, the input and output of the layer are the following:_\n",
    "\n",
    "    _The input is  ‚Ñé(ùë°)ùëó  : the hidden vector of the j-th position._\n",
    "\n",
    "    _The output is  ùëùùëó  : the probability of generating_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Cons of Encoder-Decoder Architecture___\n",
    "\n",
    "<img src='https://miro.medium.com/max/624/0*PaGt4fcpHGUUM-NA.png' width = 400/>\n",
    "\n",
    "### ___Con 1___\n",
    "_The above architecture represents the basic seq-seq model and thus they cannot be used for complex applications. The single hidden vector cannot encapsulate all the information from a sequence._\n",
    "\n",
    "####  ___Reversing the Sequence___\n",
    "* _RNNs learns much better when the input sequences are reversed._\n",
    "* _When you concatenate the source and target sequence, you can see that the words in the source sentence are far from the corresponding words in the target sentence._\n",
    "* _By reversing the source sentence, the average distance between the word in source sentence and the corresponding word in target sentence are unchanged but the first few words are now very close to each other in both source and target._\n",
    "* _This can be helpful in establishing a proper back propagation._\n",
    "\n",
    "\n",
    "### ___Con 2___\n",
    "_Neural Network needs to compress all the information of the source sentence into fixed vector length. It can be tricky during the testing time that is is difficult for NN to cope with longer sentences especially those are not in the corpus. The performance of Encoder- Decoder decreases with the increase in length of the sentences._\n",
    "\n",
    "####  ___Using Attention Mechanism___\n",
    "* _In this model, there occurs the chances of missing the importance of a word. NN cannot able to focus on the important word. It can be solved using Attention mechanism._\n",
    "* _The Attention mechanism stores the output of the previous RNNs._\n",
    "* _At each step, it ranks all the outputs by relevancy._\n",
    "* _The word with the highest score will be considered as the word to be focused on the current step._\n",
    "\n",
    "\n",
    "### ___Con 3___\n",
    "_While creating hidden vectors for longer sentences, the model doesn‚Äôt address the complexity of the grammar. Example: While predicting the output for the nth word, it considers only the 1st n-words in a sequence before the current word. But grammatically, the meaning of the word depends on words present before and after the current words in a sequence._\n",
    "\n",
    "####  ___Bi-directional LSTM___\n",
    "* _It allows to input the context of both past and future words to create encoded context vector i.e output of the encoder._\n",
    "* _At the first layer, the inputs will be given as left-to-right and at the second layer, the inputs will be given as right-to-left._\n",
    "* _The outputs from the both layers are concatenated and given as input to the third layer._\n",
    "\n",
    "<img src='https://miro.medium.com/max/700/1*D9NsJKvsOarjSHiKKLZdmQ.png'  width =400/>\n",
    "\n",
    "### ___Con 4___\n",
    "_Simply stacking the number of LSTM layers will work only to a certain amount of layers, beyond that the network result in decreased efficiency and slow training time._\n",
    "\n",
    "####  ___Adding Residual Connections___\n",
    "* _This can be solved using residual connections. The input of the current layer will be element-wise added with the output of the current layer. The added output will be given as the input to the next layer. These residual connections will make the memory states more efficient._\n",
    "\n",
    "<img src='https://miro.medium.com/max/700/1*m1o6nUHHTIkHV21y9mVbYg.png' width =400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Attention Model___\n",
    "\n",
    "_Attention models, or attention mechanisms, are input processing techniques for neural networks that allows the network to focus on specific aspects of a complex input, one at a time until the entire dataset is categorized. The goal is to break down complicated tasks into smaller areas of attention that are processed sequentially. Similar to how the human mind solves a new problem by dividing it into simpler tasks and solving them one by one._\n",
    "\n",
    "_Attention models require continuous reinforcement or backpopagation training to be effective._\n",
    "\n",
    "### ___Why Attention Model?___\n",
    "\n",
    "_The attention model was born to help memorize long source sentences in neural machine translation._\n",
    "\n",
    "_Instead of building a single context vector out of the encoder‚Äôs last hidden state, this new architecture create context vector for each input word. For example, If a source sentence has N input word, then there‚Äôs get to be with N context vectors rather than one. The advantage is that the encoded information will be much great decoded by the model._\n",
    "\n",
    "<img src='https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-attention.png' width = 600/>\n",
    "\n",
    "_Here, the Encoder generates <h1,h2,h‚Ä¶.hm> from the source sentence <X1,X2,X3‚Ä¶Xm>. So far, the architecture remains the same as the Seq2seq model. Source sentence as input and Target sentence as output. Therefore, we want to figure out the difference inside the model, which is the context vector c_{i} for each of the input word._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
