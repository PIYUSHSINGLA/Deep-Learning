{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NjGw05rg50Se"
   },
   "source": [
    "# ___Keras, a High-Level API for TensorFlow 2___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDmB3rj3gHy3"
   },
   "source": [
    "## ___What is Keras?___\n",
    "_KERAS is an Open Source Neural Network library written in Python that runs on top of Theano or Tensorflow. It is designed to be modular, fast and easy to use. It was developed by François Chollet, a Google engineer._\n",
    "\n",
    "_Keras doesn't handle low-level computation. Instead, it uses another library to do it, called the \"Backend. So Keras is high-level API wrapper for the low-level API, capable of running on top of TensorFlow, CNTK, or Theano._\n",
    "\n",
    "_Keras High-Level API handles the way we make models, defining layers, or set up multiple input-output models. In this level, Keras also compiles our model with loss and optimizer functions, training process with fit function. Keras doesn't handle Low-Level API such as making the computational graph, making tensors or other variables because it has been handled by the \"backend\" engine._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "47WODlRKgHtq"
   },
   "source": [
    "## ___What is a Backend?___\n",
    "_Backend is a term in Keras that performs all low-level computation such as tensor products, convolutions and many other things with the help of other libraries such as Tensorflow or Theano. So, the \"backend engine\" will perform the computation and development of the models. Tensorflow is the default \"backend engine\" but we can change it in the configuration._\n",
    "\n",
    "_If we want to change the backend in Keras edit our __$HOME/.Keras /Keras.json__ file and specify the different backend name such as Theano or CNTK._\n",
    "\n",
    "```{ \n",
    "   \"image_data_format\": \"channels_last\", \n",
    "   \"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"tensorflow\" \n",
    "}```\n",
    "\n",
    "_Here,_\n",
    "\n",
    "* ___image_data_format___ _represent the data format._\n",
    "\n",
    "* ___epsilon___ _represents numeric constant. It is used to avoid DivideByZero error._\n",
    "\n",
    "* ___floatx___ _represent the default data type float32. You can also change it to float16 or float64 using set_floatx() method._\n",
    "\n",
    "* ___image_data_format___ _represent the data format._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3GsIsIDgHni"
   },
   "source": [
    "## ___Advantages of Keras___\n",
    "\n",
    "* _Fast Deployment and Easy to understand_\n",
    "* _Large Community Support_\n",
    "* _Have multiple Backends: You can choose Tensorflow, CNTK, and Theano as your backend with Keras. You can choose a different backend for different projects depending on your needs. Each backend has its own unique advantage._ \n",
    "* _Cross-Platform and Easy Model Deployment_\n",
    "* _Multi GPUs Support_\n",
    "\n",
    "## ___Disadvantages of Keras___\n",
    "\n",
    "* _Cannot handle low-level API_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0sF_aN83gHfK"
   },
   "source": [
    "## ___Install Keras___\n",
    "_After we install Tensorflow, let's start installing keras. Type this command in the terminal._\n",
    "\n",
    "`pip install keras`\n",
    "\n",
    "_But with the latest version of Tensorflow, keras comes pre installed._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WjyDHthglBtk"
   },
   "source": [
    "## ___Architecture of Keras___\n",
    "\n",
    "_Keras API can be divided into three main categories:_\n",
    "\n",
    "* ___Model___\n",
    "* ___Layer___\n",
    "* ___Core Modules___\n",
    "\n",
    "<img src='https://www.tutorialspoint.com/keras/images/architecture_of_keras.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mv1PI7vTgHbk"
   },
   "source": [
    "## ___Keras Models APIs___\n",
    "\n",
    "_There are three ways to create Keras models:_\n",
    "\n",
    "* _The __Sequential model__, which is very straightforward (a simple list of layers), but is limited to single-input, single-output stacks of layers (as the name gives away) : The sequential API allows the user to create models layer-by-layer for most of the problems by using the strategy of sequential model. It does not allow the user to create models that can share the layers or have multiple inputs or outputs._\n",
    "\n",
    "* _The __Functional API__, which is an easy-to-use, fully-featured API that supports arbitrary model architectures. For most people and most use cases, this is what you should be using. This is the Keras \"industry strength\" model. : In the functional model, the functional API allows the user to create the models. These models have a lot of flexibility as the user can easily define the models where layers are connected to just the previous and next layers. User can connect the layers to any other layer in the functional model._\n",
    "\n",
    "* ___Model subclassing___ _, where you implement everything from scratch on your own. Use this if you have complex, out-of-the-box research use cases._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q0eCJbl8gHYV"
   },
   "source": [
    "## ___Keras Layers___\n",
    "_Each Keras layer in the Keras model represent the corresponding layer (input layer, hidden layer and output layer) in the actual proposed neural network model. Keras provides a lot of pre-build layers so that any complex neural network can be easily created. Some of the important Keras layers are specified below,_\n",
    "\n",
    "* ___Core Layers___\n",
    "* ___Convolution Layers___\n",
    "* ___Pooling Layers___\n",
    "* ___Recurrent Layers___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S0fWumLUgHUg"
   },
   "source": [
    "## ___Core Modules___\n",
    "_Keras also provides a lot of built-in neural network related functions to properly create the Keras model and Keras layers. Some of the function are as follows:_\n",
    "\n",
    "* ___Activations Module___ _− Activation function is an important concept in ANN and activation modules provides many activation function like softmax, relu, etc._\n",
    "\n",
    "* ___Loss Module___ _− Loss module provides loss functions like mean_squared_error, mean_absolute_error, poisson, etc._\n",
    "\n",
    "* ___Optimizer Module___ _− Optimizer module provides optimizer function like adam, sgd, etc._\n",
    "\n",
    "* ___Regularizers___ _− Regularizer module provides functions like L1 regularizer, L2 regularizer, etc._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ZqGkoktoFhw"
   },
   "source": [
    "## ___Keras - Modules___\n",
    "\n",
    "_Keras modules contains pre-defined classes, functions and variables which are useful for deep learning algorithm._\n",
    "\n",
    "* ___Initializers___ - _Provides a list of initializers function._\n",
    "\n",
    "* ___Regularizers___ - _Provides a list of regularizers function._ \n",
    " \n",
    "* ___Constraints___ - _Provides a list of constraints function._\n",
    " \n",
    "* ___Activations___ - _Provides a list of activator function._ \n",
    "\n",
    "* ___Losses___ - _Provides a list of loss function._ \n",
    "\n",
    "* ___Metrics___ - _Provides a list of metrics function._ \n",
    "\n",
    "* ___Optimizers___ - _Provides a list of optimizer function._\n",
    "\n",
    "* ___Callback___ - _Provides a list of callback function. We can use it during the training process to print the intermediate data as well as to stop the training itself (EarlyStopping method) based on some condition._\n",
    "\n",
    "* ___Text processing___ - _Provides functions to convert text into NumPy array suitable for machine learning. We can use it in data preparation phase of machine learning._\n",
    "\n",
    "* ___Image processing___ - _Provides functions to convert images into NumPy array suitable for machine learning. We can use it in data preparation phase of machine learning._\n",
    "\n",
    "* ___Sequence processing___ - _Provides functions to generate time based data from the given input data. We can use it in data preparation phase of machine learning._\n",
    "\n",
    "* ___Backend___ - _Provides function of the backend library like TensorFlow and Theano._\n",
    "\n",
    "* ___Utilities___ - _Provides lot of utility function useful in deep learning._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "as6XdN92qGkN"
   },
   "source": [
    "## ___Concept of Layers___\n",
    "\n",
    "_Keras layers are the primary building block of Keras models. Each layer receives input information, do some computation and finally output the transformed information. The output of one layer will flow into the next layer as its input._\n",
    "\n",
    "_A Keras layer requires shape of the input (input_shape) to understand the structure of the input data, initializer to set the weight for each input and finally activators to transform the output to make it non-linear. In between, constraints restricts and specify the range in which the weight of input data to be generated and regularizer will try to optimize the layer (and the model) by dynamically applying the penalties on the weights during optimization process._\n",
    "\n",
    "_To summarise, Keras layer requires below minimum details to create a complete layer:_\n",
    "\n",
    "* ___Shape of the Input Data___\n",
    "* ___Number of Neurons/Units in the Layer___\n",
    "* ___Initializers___\n",
    "* ___Regularizers___\n",
    "* ___Constraints___\n",
    "* ___Activations___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kctkcZcYxdJy"
   },
   "source": [
    "### ___Types of Layers___\n",
    "\n",
    "* ___Dense Layer___\n",
    "\n",
    "  _Dense layer is the regular deeply connected neural network layer._\n",
    "\n",
    "  _The argument supported by Dense layer is as follows:_\n",
    "\n",
    "  * ___units___ _represent the number of units and it affects the output layer._\n",
    "  * ___activation___ _represents the activation function._\n",
    "  * ___use_bias___ _represents whether the layer uses a bias vector._\n",
    "  * ___kernel_initializer___ _represents the initializer to be used for kernel._\n",
    "  * ___bias_initializer___ _represents the initializer to be used for the bias vector._\n",
    "  * ___kernel_regularizer___ _represents the regularizer function to be applied to the kernel weights matrix._\n",
    "  * ___bias_regularizer___ _represents the regularizer function to be applied to the bias vector._\n",
    "  * ___activity_regularizer___ _represents the regularizer function tp be applied to the output of the layer._\n",
    "  * ___kernel_constraint___ _represent constraint function to be applied to the kernel weights matrix._\n",
    "  * ___bias_constraint___ _represent constraint function to be applied to the bias vector._\n",
    "\n",
    "  _As you have seen, there is no argument available to specify the_ ___input_shape___ _of the input data._ ___input_shape___ _is a special argument, which the layer will accept only if it is designed as first layer in the model._\n",
    "\n",
    "* ___Dropout Layers___\n",
    "\n",
    "  _Dropout is one of the important concept in the Deep learning for Active Neurons._\n",
    "\n",
    "  _It is used to fix the over-fitting issue. Input data may have some of the unwanted data, usually called as Noise. Dropout will try to remove the noise data and thus prevent the model from over-fitting._\n",
    "\n",
    "   ```keras.layers.Dropout(rate, noise_shape = None, seed = None)```\n",
    "\n",
    "  * ___rate___ _− represent the fraction of the input unit to be dropped. It will be from 0 to 1._\n",
    "  * ___noise_shape___ _represent the dimension of the shape in which the dropout to be applied. For example, the input shape is (batch_size, timesteps, features). Then, to apply dropout in the timesteps, (batch_size, 1, features) need to be specified as noise_shape._\n",
    "  * ___seed___ _− random seed._\n",
    "\n",
    "* ___Flatten Layers___\n",
    "\n",
    "  _Flatten is used to flatten the input. For example, if flatten is applied to layer having input shape as (batch_size, 2,2), then the output shape of the layer will be (batch_size, 4)_\n",
    "\n",
    "  ```keras.layers.Flatten(data_format = None)```\n",
    "  * ___data_format___ _is an optional argument and it is used to preserve weight ordering when switching from one data format to another data format. It accepts either ___channels_last___ or ___channels_first___ as value. channels_last is the default one and it identifies the input shape as (batch_size, ..., channels) whereas channels_first identifies the input shape as (batch_size, channels, ...)_\n",
    "\n",
    "* ___Reshape Layers___\n",
    "\n",
    "  _Reshape is used to change the shape of the input._\n",
    "\n",
    "* ___Permute Layers___\n",
    "\n",
    "  _Permute is also used to change the shape of the input using pattern._\n",
    "\n",
    "* ___RepeatVector Layers___\n",
    "\n",
    "  _RepeatVector is used to repeat the input for set number, n of times._\n",
    "\n",
    "* ___Lambda Layers___\n",
    "\n",
    "  _Lambda is used to transform the input data using an expression or function._\n",
    "\n",
    "* ___Convolution Layers___\n",
    "\n",
    "  _Keras contains a lot of layers for creating Convolution based ANN, popularly called as Convolution Neural Network (CNN)._\n",
    "\n",
    "* ___Pooling Layer___\n",
    "\n",
    "  _It is used to perform max pooling operations on temporal data._\n",
    "\n",
    "* ___Locally connected Layer___\n",
    "\n",
    "  _Locally connected layers are similar to Conv1D layer but the difference is Conv1D layer weights are shared but here weights are unshared._\n",
    "\n",
    "* ___Merge Layer___\n",
    "\n",
    "  _It is used to merge a list of inputs._\n",
    "\n",
    "* ___Embedding Layer___\n",
    "\n",
    "  _It performs embedding operations in input layer. It is used to convert positive into dense vectors of fixed size. Its main application is in text analysis._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZeaDDdjl_BZ"
   },
   "source": [
    "### ___Input Shape___\n",
    "_In deep learning, all type of input data like text, images or videos will be first converted into array of numbers and then feed into the algorithm. Input numbers may be single dimensional array, two dimensional array (matrix) or multi-dimensional array. We can specify the dimensional information using shape, a tuple of integers. To create the first layer of the model (or input layer of the model), shape of the input data should be specified._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rh1Zfp2oxjUK"
   },
   "source": [
    "### ___Initializers___\n",
    "_Initializers module provides different functions to set these initial weight. Some of the Keras Initializer function are as follows:_\n",
    "\n",
    "`from keras import initializers`\n",
    "\n",
    "* ___Zeros___\n",
    "\n",
    "```Dense(512, activation = 'relu', input_shape = (784,), kernel_initializer = initializers.Zeros())```\n",
    "\n",
    "* ___Ones___\n",
    "\n",
    "```Dense(512, activation = 'relu', input_shape = (784,), kernel_initializer = initializers.Ones())```\n",
    "\n",
    "* ___Constant___\n",
    "\n",
    "```Dense(512, activation = 'relu', input_shape = (784,), kernel_initializer = initializers.Constant(value = 0))```\n",
    "\n",
    "* ___RandomNormal___\n",
    "\n",
    "```Dense(512, activation = 'relu', input_shape = (784,), kernel_initializer = initializers..RandomNormal(mean=0.0, stddev = 0.05, seed = None))```\n",
    "\n",
    "* ___RandomUniform___\n",
    "\n",
    "```Dense(512, activation = 'relu', input_shape = (784,), kernel_initializer = RandomUniform(minval = -0.05, maxval = 0.05, seed = None))```\n",
    "\n",
    "* ___TruncatedNormal___\n",
    "\n",
    "```Dense(512, activation = 'relu', input_shape = (784,), kernel_initializer = TruncatedNormal(mean = 0.0, stddev = 0.05, seed = None))```\n",
    "\n",
    "* ___GlorotNormal___\n",
    "\n",
    "```Dense(512, activation = 'relu', input_shape = (784,), kernel_initializer = glorot_normal(seed=None))```\n",
    "\n",
    "$stddev = sqrt(2 / (fan_in + fan_out))$\n",
    "\n",
    "* ___GlorotUniform___\n",
    "\n",
    "_It draws samples from a uniform distribution within -limit, limit where limit is $sqrt(6 / (fan_in + fan_out))$ where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor._\n",
    "\n",
    "```Dense(512, activation = 'relu', input_shape = (784,), kernel_initializer = glorot_uniform(seed = None))```\n",
    "\n",
    "* ___he_normal___\n",
    "_Generates value using he normal distribution of input data._\n",
    "\n",
    "```Dense(512, activation = 'relu', input_shape = (784,), kernel_initializer = he_normal(seed = None))```\n",
    "\n",
    "$stddev = sqrt(2 / fan_in)$\n",
    "\n",
    "_works best with Relu_\n",
    "\n",
    "* ___he_uniform___\n",
    "_Generates value using he uniform distribution of input data._\n",
    "\n",
    "```Dense(512, activation = 'relu', input_shape = (784,), kernel_initializer = he_uniform(seed = None))```\n",
    "\n",
    "$stddev = sqrt(6 / fan_in)$\n",
    "\n",
    "_works best with Relu_\n",
    "\n",
    "* ___Identity___\n",
    "\n",
    "```Dense(512, activation = 'relu', input_shape = (784,), kernel_initializer = Identity(gain = 1.0))```\n",
    "\n",
    "* ___Orthogonal___\n",
    "\n",
    "```Dense(512, activation = 'relu', input_shape = (784,), kernel_initializer = Orthogonal(gain = 1.0, seed = None))```\n",
    "\n",
    "* ___VarianceScaling___\n",
    "\n",
    "_Generates value based on the input shape and output shape of the layer along with the specified scale._\n",
    "\n",
    "```Dense(512, activation = 'relu', input_shape = (784,), kernel_initializer = VarianceScaling(scale = 1.0, mode = 'fan_in', distribution = 'normal', seed = None))```\n",
    "\n",
    "_where,_\n",
    "  * _scale represent the scaling factor_\n",
    "  * _mode represent any one of fan_in, fan_out and fan_avg values_\n",
    "  * _distribution represent either of normal or uniform_\n",
    "\n",
    "_It finds the stddev value for normal distribution using below formula and then find the weights using normal distribution,_\n",
    "\n",
    "$stddev = sqrt(scale / n)$\n",
    "\n",
    "_where n represent,_\n",
    "  * _number of input units for mode = fan_in_\n",
    "  * _number of out units for mode = fan_out_\n",
    "  * _average number of input and output units for mode = fan_avg_\n",
    "\n",
    "_Similarly, it finds the limit for uniform distribution using below formula and then find the weights using uniform distribution,_\n",
    "\n",
    "$limit = sqrt(3 * scale / n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8D88E09ixptB"
   },
   "source": [
    "### ___Constraints___\n",
    "\n",
    "_A constraint will be set on the parameter (weight) during optimization phase. Constraints module provides different functions to set the constraint on the layer. Some of the constraint functions are as follows:_\n",
    "\n",
    "`from keras import constraints `\n",
    "\n",
    "* ___NonNeg___\n",
    "\n",
    "_Constrains weights to be non-negative._\n",
    "\n",
    "```model.add(Dense(512, activation = 'relu', input_shape = (784,), kernel_constraint = constraints.NonNeg()))```\n",
    "\n",
    "* ___UnitNorm___\n",
    "\n",
    "_Constrains weights to be unit norm._\n",
    "\n",
    "```model.add(Dense(512, activation = 'relu', input_shape = (784,), kernel_constraint = constraints.UnitNorm(axis = 0)))```\n",
    "\n",
    "* ___MaxNorm___\n",
    "\n",
    "_Constrains weight to norm less than or equals to the given value._\n",
    "\n",
    "```model.add(Dense(512, activation = 'relu', input_shape = (784,), kernel_constraint = constraints.MaxNorm(max_value = 2, axis = 0)))```\n",
    "\n",
    "  * _max_value represent the upper bound_\n",
    "  * _axis represent the dimension in which the constraint to be applied. e.g. in Shape (2,3,4) axis 0 denotes first dimension, 1 denotes second dimension and 2 denotes third dimension_\n",
    "\n",
    "* ___MinMaxNorm___\n",
    "\n",
    "_Constrains weights to be norm between specified minimum and maximum values._\n",
    "\n",
    "```model.add(Dense(512, activation = 'relu', input_shape = (784,), kernel_constraint = constraints.MinMaxNorm(min_value = 0.0, max_value = 1.0, rate = 1.0, axis = 0)))```\n",
    "\n",
    "  * _where, rate represent the rate at which the weight constrain is applied._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4N7LXC8Dxvyo"
   },
   "source": [
    "### ___Regularizers___\n",
    "_Regularizers are used in the optimization phase. It applies some penalties on the layer parameter during optimization. Keras regularization module provides below functions to set penalties on the layer. Regularization applies per-layer basis only._\n",
    "\n",
    "* ___L1 Regularizer___\n",
    "_It provides L1 based regularization._\n",
    "\n",
    "```model.add(Dense(512, activation = 'relu', input_shape = (784,), kernel_regularizer=regularizers.l1(l1=0.01)))```\n",
    "\n",
    "* ___L2 Regularizer___\n",
    "_It provides L2 based regularization._\n",
    "\n",
    "```model.add(Dense(512, activation = 'relu', input_shape = (784,), kernel_regularizer=regularizers.l2(l2=0.01)))```\n",
    "\n",
    "* ___L1 and L2 Regularizer___\n",
    "_It provides both L1 and L2 based regularization._\n",
    "\n",
    "```model.add(Dense(512, activation = 'relu', input_shape = (784,), kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)))```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4xYhLWyWxyga"
   },
   "source": [
    "### ___Activations___\n",
    "\n",
    "_The Activation function does a nonlinear transformation of the input data and thus enable the neurons to learn better. Output of a neuron depends on the activation function._\n",
    "\n",
    "```from keras.layers import Activation```\n",
    "\n",
    "```model.add(Dense(512, activation = 'exponential', input_shape = (784,)))```\n",
    "\n",
    "___activation = exponential, relu, softmax, elu, sigmoid, tanh, softplus, linear, etc.___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LnbjTdF3xSG7"
   },
   "source": [
    "## ___Concept of Models___\n",
    "\n",
    "_Keras model represents the actual neural network model. Keras provides a two mode to create the model, simple and easy to use Sequential API as well as more flexible and advanced Functional API._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0qE4kzvu50Xm"
   },
   "source": [
    "### ___Sequential Model___\n",
    "\n",
    "_To build a Keras Sequential model, you add layers to it in the same order that you want the computations to be undertaken by the network._\n",
    "\n",
    "_After you have built your model, you compile it; this optimizes the computations that are to be undertaken, and is where you allocate the optimizer and the loss function you want your model to use._\n",
    "\n",
    "_The next stage is to fit the model to the data. This is commonly known as training the model, and is where all the computations take place. It is possible to present the data to the model either in batches, or all at once._\n",
    "\n",
    "_Next, you evaluate your model to establish its accuracy, loss, and other metrics. Finally, having trained your model, you can use it to make predictions on new data. So, the workflow is: build, compile, fit, evaluate, make predictions._\n",
    "\n",
    "```from keras.models import Sequential```\n",
    "\n",
    "_There are two ways to create a Sequential model. Let's take a look at each of them._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJUmMWml50Xm"
   },
   "source": [
    "#### ___Using Sequential Model___\n",
    "\n",
    "_Firstly, you can pass a list of layer instances to the constructor, as in the following example. For now, we will just explain enough to allow you to understand what is happening here._\n",
    "\n",
    "_Acquire the data. MNIST is a dataset of hand-drawn numerals, each on a 28 x 28 pixel grid. Every individual data point is an unsigned 8-bit integer (uint8), as are the labels:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wOiwocCV50Xn"
   },
   "source": [
    "##### _Loading the Dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "9JbTUwq550Xn",
    "outputId": "00350d94-5a5a-4d8d-ed45-70a223168794"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "# Load Dataset from Keras inbuilt Datasets\n",
    "mnist_data = tf.keras.datasets.mnist\n",
    "\n",
    "(train_x,train_y), (test_x, test_y) = mnist_data.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "btP8iumX50Xp"
   },
   "source": [
    "##### _Defining the Variables_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZqnaGaUH50Xq"
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NO01BLO5zKJK"
   },
   "source": [
    "___Mini-batching___\n",
    "\n",
    "_Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time. This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset._\n",
    "\n",
    "_Mini-batching is computationally inefficient, since you can't calculate the loss simultaneously across all samples. However, this is a small price to pay in order to be able to run the model at all._\n",
    "\n",
    "_It's also quite useful combined with SGD. The idea is to randomly shuffle the data at the start of each epoch, then create the mini-batches. For each mini-batch, you train the network weights with gradient descent. Since these batches are random, you're performing SGD with each batch._\n",
    "\n",
    "___Epochs___\n",
    "\n",
    "_An epoch is a single forward and backward pass of the whole dataset. This is used to increase the accuracy of the model without requiring more data._\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QSK_KwYM50Xs"
   },
   "outputs": [],
   "source": [
    "# normalize all the data points and cast the labels to int64\n",
    "\n",
    "train_x, test_x = tf.cast(train_x/255.0, tf.float32), tf.cast(test_x/255.0, tf.float32)\n",
    "train_y, test_y = tf.cast(train_y,tf.int64),tf.cast(test_y,tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ai-FPXpp0Sxa"
   },
   "source": [
    "#### ___Sequential Model 1___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z_hYJi4r50Xx"
   },
   "source": [
    "##### _Building the Architecture_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Be82W4QA50Xx"
   },
   "outputs": [],
   "source": [
    "# Sequential Model 1\n",
    "\n",
    "mnistmodel1 = tf.keras.models.Sequential([\n",
    "tf.keras.layers.Flatten(),\n",
    "tf.keras.layers.Dense(512,activation=tf.nn.relu),\n",
    "tf.keras.layers.Dropout(0.2),\n",
    "tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yoSzTE2X50Xz"
   },
   "source": [
    "##### _Compiling the Model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Smz8trOS50X0"
   },
   "outputs": [],
   "source": [
    "# choosing Optimizer for backpropogation\n",
    "optimiser = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Compiling model with loss function , optimizer and metrics\n",
    "mnistmodel1.compile(optimizer= optimiser, loss='sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "050bQ56A2PPF"
   },
   "source": [
    "___Loss___\n",
    "\n",
    "_Loss function is used to find error or deviation in the learning process. Keras requires loss function during model compilation process._\n",
    "\n",
    "_Keras provides quite a few loss function in the losses module and they are as follows:_\n",
    "\n",
    "* ___mean_squared_error___\n",
    "* ___mean_absolute_error___\n",
    "* ___mean_absolute_percentage_error___\n",
    "* ___mean_squared_logarithmic_error___\n",
    "* ___squared_hinge___\n",
    "* ___hinge___\n",
    "* ___categorical_hinge___\n",
    "* ___logcosh___\n",
    "* ___huber_loss___\n",
    "* ___categorical_crossentropy___\n",
    "* ___sparse_categorical_crossentropy___\n",
    "* ___binary_crossentropy___\n",
    "* ___kullback_leibler_divergence___\n",
    "* ___poisson___\n",
    "* ___cosine_proximity___\n",
    "* ___is_categorical_crossentropy___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MxcEhC9L2jO1"
   },
   "source": [
    "___Metrics___\n",
    "\n",
    "_Metrics is used to evaluate the performance of your model. It is similar to loss function, but not used in training process. Keras provides quite a few metrics as a module, metrics and they are as follows:_\n",
    "\n",
    "* ___accuracy___\n",
    "* ___binary_accuracy___\n",
    "* ___categorical_accuracy___\n",
    "* ___sparse_categorical_accuracy___\n",
    "* ___top_k_categorical_accuracy___\n",
    "* ___sparse_top_k_categorical_accuracy___\n",
    "* ___cosine_proximity___\n",
    "* ___clone_metric___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ad_o9-A50X6"
   },
   "source": [
    "##### _Fitting the Model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "IpcoACWl50X7",
    "outputId": "e90243ef-5807-4c9c-a84e-21c50212289a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2203 - accuracy: 0.9339\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0966 - accuracy: 0.9702\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0693 - accuracy: 0.9789\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0540 - accuracy: 0.9834\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0445 - accuracy: 0.9851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f62e52fa668>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnistmodel1.fit(train_x, train_y, batch_size= batch_size, epochs= epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tPney75P50X_"
   },
   "source": [
    "##### _Evaluate the mnistmodel1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "wxJ0qMax50YB",
    "outputId": "bed3d3fa-1fc9-4029-b26a-0585d9be04d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0655 - accuracy: 0.9804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06554202735424042, 0.980400025844574]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnistmodel1.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WNHdY9Ib50YH"
   },
   "source": [
    "_This represents a loss of 0.06 and an accuracy of 0.9801 on the test data._\n",
    "\n",
    "_An accuracy of 0.98 means that out of 100 test data points, 98 were, on average, correctly identified by the model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "9JpN4lU53PZ_",
    "outputId": "055156b8-2760-48f6-abf5-26bba6ffbdf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnistmodel1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HlEKIiFv0ZaL"
   },
   "source": [
    "#### ___Sequential Model 2___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vOpTYf_K50YN"
   },
   "source": [
    "##### _Building the Architecture & Compiling_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAQgttwO50YO"
   },
   "outputs": [],
   "source": [
    "mnistmodel2 = tf.keras.models.Sequential();\n",
    "\n",
    "mnistmodel2.add(tf.keras.layers.Flatten())\n",
    "mnistmodel2.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "mnistmodel2.add(tf.keras.layers.Dropout(0.2))\n",
    "mnistmodel2.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax))\n",
    "\n",
    "mnistmodel2.compile(optimizer= tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1qrGU-oP50YR"
   },
   "source": [
    "##### _Fitting the mnistmodel2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "bYT4vyS150YS",
    "outputId": "fdd85bec-f29e-4106-e3a9-6ec681220c07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.2462 - accuracy: 0.9285\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1069 - accuracy: 0.9682\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.0730 - accuracy: 0.9781\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.0576 - accuracy: 0.9815\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.0436 - accuracy: 0.9868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f62ec150630>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnistmodel2.fit(train_x, train_y, batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wJ2U1J2G50YV"
   },
   "source": [
    "##### _Evaluate the mnistmodel2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "5KJKWQia50YY",
    "outputId": "d88dcac7-ff7e-4b8a-8ba3-b8c88a3ecf22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0642 - accuracy: 0.9809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06417829543352127, 0.98089998960495]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnistmodel2.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7AmYeKMh50Yd"
   },
   "source": [
    "### ___Functional API___\n",
    "\n",
    "_The functional API lets you build much more complex architectures than the simple linear stack of Sequential models we have seen previously. It also supports more advanced models. These models include multi-input and multi-output models, models with shared layers, and models with residual connections._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A4H2yy1N50Yg"
   },
   "source": [
    "##### _Building the Architecture_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFMgF36i50Yh"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(28,28)) # Returns a 'placeholder' tensor\n",
    "\n",
    "x = tf.keras.layers.Flatten()(inputs)\n",
    "x = tf.keras.layers.Dense(512, activation='relu',name='d1')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "predictions = tf.keras.layers.Dense(10,activation=tf.nn.softmax, name='d2')(x)\n",
    "\n",
    "mnistmodel3 = tf.keras.Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N7KBEAhH50Yk"
   },
   "source": [
    "##### _Compile & Fit_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "iNYFvrTw50Yk",
    "outputId": "d3b1f99b-6d01-4d7f-ca1d-4401219ba818"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2190 - accuracy: 0.9350\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0970 - accuracy: 0.9703\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0669 - accuracy: 0.9789\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0530 - accuracy: 0.9829\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0415 - accuracy: 0.9865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f62e12bee48>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimiser = tf.keras.optimizers.Adam()\n",
    "\n",
    "mnistmodel3.compile(optimizer= optimiser, loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "mnistmodel3.fit(train_x, train_y, batch_size=32, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "anfKYnio50Yn"
   },
   "source": [
    "##### _Evaluate the mnistmodel3_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "JWby1LNV50Yo",
    "outputId": "33ad58cf-23e1-4ea4-ad68-df11994486f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0597 - accuracy: 0.9827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.059650298207998276, 0.982699990272522]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnistmodel3.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i5My3rrb50Yq"
   },
   "source": [
    "### ___Subclassing the Keras Model___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iaJCh3Li50Ys"
   },
   "source": [
    "##### _Building the Subclass Architecture_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLJKSWRc50Yt"
   },
   "outputs": [],
   "source": [
    "class MNISTModel(tf.keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        # Define your layers here.\n",
    "        inputs = tf.keras.Input(shape=(28,28)) # Returns a placeholder tensor\n",
    "        self.x0 = tf.keras.layers.Flatten()\n",
    "        self.x1 = tf.keras.layers.Dense(512, activation='relu',name='d1')\n",
    "        self.x2 = tf.keras.layers.Dropout(0.2)\n",
    "        self.predictions = tf.keras.layers.Dense(10,activation=tf.nn.softmax, name='d2')\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "    # This is where to define your forward pass\n",
    "    # using the layers previously defined in `__init__`\n",
    "        x = self.x0(inputs)\n",
    "        x = self.x1(x)\n",
    "        x = self.x2(x)\n",
    "        return self.predictions(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "enFRzZti50Yu"
   },
   "outputs": [],
   "source": [
    "mnistmodel4 = MNISTModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "97Pzn3qq50Yw"
   },
   "source": [
    "##### _Compile & Fit_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "QNbsgFWI50Yx",
    "outputId": "81c2296a-d05e-43f7-bac7-e6e3a1a4e73d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2212 - accuracy: 0.9346\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0963 - accuracy: 0.9698\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0702 - accuracy: 0.9779\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0539 - accuracy: 0.9828\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0438 - accuracy: 0.9864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f62e0155c18>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "steps_per_epoch = len(train_x.numpy())//batch_size\n",
    "print(steps_per_epoch)\n",
    "\n",
    "mnistmodel4.compile (optimizer= tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy',metrics = ['accuracy'])\n",
    "\n",
    "mnistmodel4.fit(train_x, train_y, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bDYvNsB50Yz"
   },
   "source": [
    "##### _Evaluate the mnistmodel4_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "6xXpyT9S50Yz",
    "outputId": "ec5fcb73-58f1-4dc7-fbf9-078d009b6fba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0649 - accuracy: 0.9812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0648932158946991, 0.9811999797821045]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnistmodel4.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0N3rAolaX2U"
   },
   "source": [
    "##### ___Model Prediction___\n",
    "_Prediction is the final step and our expected outcome of the model generation. Keras provides a method, predict to get the prediction of the trained model. The signature of the predict method is as follows,_\n",
    "\n",
    "`predict(\n",
    "   x, \n",
    "   batch_size = None, \n",
    "   verbose = 0, \n",
    "   steps = None, \n",
    "   callbacks = None, \n",
    "   max_queue_size = 10, \n",
    "   workers = 1, \n",
    "   use_multiprocessing = False\n",
    ")`\n",
    "\n",
    "_Here, all arguments are optional except the first argument, which refers the unknown input data. The shape should be maintained to get the proper prediction._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GIQJ7lEE7v4e"
   },
   "source": [
    "### ___Saving and Loading Keras Models___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ByRFQlOk7v4f"
   },
   "source": [
    "_The Keras API in TensorFlow has the ability to save and restore models easily. This is done as follows, and saves the model in the current directory. Of course, a longer path may be passed here:_\n",
    "\n",
    "#### ___Saving a model___\n",
    "    \n",
    "`model.save('./model_name.h5')`\n",
    "\n",
    "_This will save the model architecture, its weights, its training state (loss, optimizer), and the state of the optimizer, so that you can carry on training the model from where you left off._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IWp2Rx7F7v4g"
   },
   "source": [
    "_Loading a saved model is done as follows. Note that if you have compiled your model, the load will compile your model using the saved training configuration:_\n",
    "\n",
    "#### ___Loading a model___\n",
    "\n",
    "`from tensorflow.keras.models import load_model`\n",
    "\n",
    "`new_model = load_model('./model_name.h5')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b94iFWCTYBfJ"
   },
   "source": [
    "_It is also possible to save just the architecture of the model._\n",
    "\n",
    "#### ___Saving the Model Architecture___\n",
    "\n",
    "`json_string = model.to_json()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HaVrkRg2Ynrs"
   },
   "source": [
    "_To Load the saved Architecture_\n",
    "\n",
    "#### ___Load Model Architecture___\n",
    "\n",
    "`from tensorflow.keras.models import model_from_json`\n",
    "\n",
    "`model_architecture = model_from_json(json_string)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jf-pVhbM7v4i"
   },
   "source": [
    "_It is also possible to save just the model weights and load them with this (in which case, you must build your architecture to load the weights into):_\n",
    "\n",
    "#### ___Saving the Model Weights___\n",
    "    \n",
    "`model.save_weights('./model_weights.h5')`\n",
    "    \n",
    "_Then use the following to load it:_\n",
    "\n",
    "#### ___Loding the Weights___\n",
    "    \n",
    "`model.load_weights('./model_weights.h5')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9fIWQAw7v6c"
   },
   "source": [
    "### ___One-Hot Encoding___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YF1YP4ow7v6c"
   },
   "source": [
    "_One-hot encoding (OHE) is where a tensor is constructed from the data labels with a 1 in each of the elements corresponding to a label's value, and 0 everywhere else; that is, one of the bits in the tensor is hot (1)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UD4pEAG-7v6d"
   },
   "source": [
    "#### ___One-Hot Encoding 1___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9MeRehzb7v6d"
   },
   "source": [
    "_In this example, we are converting a decimal value of 7 to a one-hot encoded value of 0000000100 using_\n",
    "\n",
    "`tf.one_hot()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "qXFYxAfj7v6e",
    "outputId": "ebd73878-61eb-489d-811b-66a7cba78e70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 is  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] when one-hot encoded with a depth of 10\n"
     ]
    }
   ],
   "source": [
    "z = 7\n",
    "\n",
    "z_train_ohe = tf.one_hot(z, depth=10).numpy()\n",
    "\n",
    "print(z, \"is \",z_train_ohe,\"when one-hot encoded with a depth of 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2vQdZjnG7v6k"
   },
   "source": [
    "#### ___One-Hot Encoding 2___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1OWHZht7v6l"
   },
   "source": [
    "_Using the fashion MNIST dataset._\n",
    "\n",
    "_The original labels are integers from 0 to 9, so, for example, a label of 5 becomes 0000010000 when onehot encoded, but note the difference between the index and the label stored at that index:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "colab_type": "code",
    "id": "P0mCg2nQ7v6m",
    "outputId": "3a453758-60d3-4fe0-9b46-b20cf0febe01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "width, height, = 28,28\n",
    "\n",
    "# total classes\n",
    "n_classes = 10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uokUtvRc7v63"
   },
   "outputs": [],
   "source": [
    "split = 50000\n",
    "(y_train, y_valid) = y_train[:split], y_train[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "HMGexQQz7v67",
    "outputId": "4235b881-7705-4b93-ba40-014bad5373fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_train_ohe = tf.one_hot(y_train, depth=n_classes).numpy()\n",
    "y_valid_ohe = tf.one_hot(y_valid, depth=n_classes).numpy()\n",
    "y_test_ohe = tf.one_hot(y_test, depth=n_classes).numpy()\n",
    "\n",
    "# show difference between the original label and a one-hot-encoded label\n",
    "i=8\n",
    "print(y_train[i]) # 'ordinary' number value of label at index i=8 is 5\n",
    "\n",
    "# note the difference between the index of 8 and the label at that index which is 5\n",
    "print(y_train_ohe[i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U6_0A6yA7v4j"
   },
   "source": [
    "## ___Keras Datasets Module___\n",
    "\n",
    "_Before creating a model, we need to choose a problem, need to collect the required data and convert the data to NumPy array. Once data is collected, we can prepare the model and train it by using the collected data. Data collection is one of the most difficult phase of machine learning. Keras provides a special module, datasets to download the online machine learning data for training purposes. It fetches the data from online server, process the data and return the data as training and test set._\n",
    "\n",
    "_The data available in the module are as follows:_\n",
    "\n",
    "  * ___CIFAR10 small image classification___\n",
    "  * ___CIFAR100 small image classification___\n",
    "  * ___IMDB Movie reviews sentiment classification___\n",
    "  * ___Reuters newswire topics classification___\n",
    "  * ___MNIST database of handwritten digits___\n",
    "  * ___Fashion-MNIST database of fashion articles___\n",
    "  * ___Boston housing price regression dataset___\n",
    "\n",
    "_They are all accessed with the function:_\n",
    "\n",
    "`load_data()`  \n",
    "\n",
    "_For example, to load the fashion_mnist dataset, use the following:_\n",
    "\n",
    "`(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gjEMFhy_bfAn"
   },
   "source": [
    "## ___Options to Load Data into TensorFlow___\n",
    "_The first step before training a machine learning algorithm is to load the data. There is two commons way to load data:_\n",
    "\n",
    "1. ___Load data into memory___ _: It is the simplest method. You load all your data into memory as a single array. You can write a Python code. This lines of code are unrelated to Tensorflow._\n",
    "\n",
    "2. ___Tensorflow Data Pipeline___ _: Tensorflow has built-in API that helps you to load the data, perform the operation and feed the machine learning algorithm easily. This method works very well especially when you have a large dataset. For instance, image records are known to be enormous and do not fit into memory. The data pipeline manages the memory by itself._\n",
    "\n",
    "### ___Load Data in Memory___\n",
    "\n",
    "_If your dataset is not too big, i.e., less than 10 gigabytes, you can use the first method. The data can fit into the memory. You can use a famous library called Pandas to import CSV files._\n",
    "\n",
    "### ___Load Data with Tensorflow Pipeline___\n",
    "\n",
    "_The second method works best if you have a large dataset. For instance, if you have a dataset of 50 gigabytes, and your computer has only 16 gigabytes of memory then the machine will crash._\n",
    "\n",
    "_In this situation, you need to build a Tensorflow pipeline. The pipeline will load the data in __batch__, or small chunk. Each batch will be pushed to the pipeline and be ready for the training. Building a pipeline is an excellent solution because it allows you to use parallel computing. It means Tensorflow will train the model across multiple CPUs. It fosters the computation and permits for training powerful neural network._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cwcii7Ts3u_p"
   },
   "source": [
    "#### ___How to create Datasets?___\n",
    "_Tensorflow provides various methods to create Datasets from numpy arrays, text files, CSV files, tensors, etc. Let’s look at few methods below:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d1IbHZcv4A_q"
   },
   "source": [
    "#### ___from_tensor_slices___ \n",
    "\n",
    "_It accepts single or multiple numpy arrays or tensors. Dataset created using this method will emit only one data at a time._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "NtIFVtS42YAG",
    "outputId": "801744f7-1ca2-4f4f-cba4-9b07b66b5737"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int64>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# source data - numpy array\n",
    "data = np.arange(10)\n",
    "\n",
    "# create a dataset from numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "colab_type": "code",
    "id": "AxckIaNzRtet",
    "outputId": "6d852c5b-bf70-4cbd-d984-5e30610fce89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for elem in dataset:\n",
    "  print(elem.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "fdAhy7cORxis",
    "outputId": "3112308a-89b2-401f-fa8a-6d02af0c2370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "it = iter(dataset)\n",
    "print(next(it).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "hJh1-0GzR6a6",
    "outputId": "6d8b682a-b05d-4e96-b1d7-d234487d0fe7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (10,), types: tf.float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10]))\n",
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "colab_type": "code",
    "id": "owFgQfJlTJoj",
    "outputId": "017a278c-1666-4762-8534-d05297c71dcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41135347 0.03141069 0.54174507 0.28996527 0.51327515 0.25448108\n",
      " 0.30541623 0.12145805 0.87665796 0.4771775 ]\n",
      "[0.9282093  0.92357934 0.71846044 0.03223097 0.94496965 0.7557212\n",
      " 0.49731612 0.16906571 0.33520496 0.0521158 ]\n",
      "[0.92983496 0.04271567 0.3680334  0.51463735 0.6034411  0.9591875\n",
      " 0.30914724 0.61624277 0.74324155 0.6654117 ]\n",
      "[0.7436255  0.23171341 0.9662912  0.85830724 0.37194717 0.13710117\n",
      " 0.6405338  0.2892282  0.13706768 0.06933939]\n"
     ]
    }
   ],
   "source": [
    "iterator = tf.compat.v1.data.make_one_shot_iterator(dataset1)\n",
    "\n",
    "for item in dataset1:\n",
    "    number = iterator.get_next().numpy()\n",
    "    print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "kHQksT-8SD4q",
    "outputId": "abfbd7f2-8941-4bfd-9942-cfa52aed6dc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(100,), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random.uniform([4]),\n",
    "    tf.random.uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "\n",
    "dataset2.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4tQjJpSK4LAT"
   },
   "source": [
    "#### ___from_tensors___ \n",
    "\n",
    "_It also accepts single or multiple numpy arrays or tensors. Dataset created using this method will emit all the data at once._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "2ce7qGWK2X2l",
    "outputId": "47010f28-9c24-4920-da54-7894fe0c4c2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tf.constant(10)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensors(data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "o3SUHzkdi3pJ",
    "outputId": "5418635f-1bfc-4218-9f68-fac7c4ec4756"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorDataset shapes: (3, 4), types: tf.int32>"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset containing a sparse tensor.\n",
    "dataset4 = tf.data.Dataset.from_tensors(tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4]))\n",
    "dataset4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "pWhBYT4Vi_AS",
    "outputId": "f18bc1e2-4682-4e1a-cfe8-1f5e21b0fc79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "iterator = tf.compat.v1.data.make_one_shot_iterator(dataset4)\n",
    "\n",
    "for item in dataset4:\n",
    "    number = iterator.get_next().values.numpy()\n",
    "    print(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVqpNnRF4Ruv"
   },
   "source": [
    "#### ___from_generator___ \n",
    "\n",
    "_Creates a Dataset whose elements are generated by a function._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "O_w7nTB72X0Q",
    "outputId": "e1a639b1-21f0-41cc-fde2-1399ff7f75ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset shapes: <unknown>, types: tf.int32>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generator():\n",
    "  for i in range(10):\n",
    "    yield 2*i\n",
    "    \n",
    "dataset = tf.data.Dataset.from_generator(generator, (tf.int32))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LpRJSV3_ZM4y"
   },
   "source": [
    "#### ___Operations___\n",
    "\n",
    "___Operations to apply to the data, these include operations such as:___\n",
    "\n",
    "* ___batch()___ _– this allows you to consume the data from your TensorFlow Dataset in batches_\n",
    "* ___map()___ _– this allows you to transform the data using lambda statements applied to each element_\n",
    "* ___zip()___ _– this allows you to zip together different Dataset objects into a new Dataset, in a similar way to the Python zip function_\n",
    "* ___filter()___ _– this allows you to remove problematic data-points in your data-set, again based on some lambda function_\n",
    "* ___repeat()___ _– this operation restricts the number of times data is consumed from the Dataset before a tf.errors.OutOfRangeError error is thrown means that the dataset will be re-fed from the beginning when its end is reached (continuously)_\n",
    "* ___shuffle()___ _– this operation shuffles the data in the Dataset_\n",
    "\n",
    "_[Reference](https://www.kaggle.com/jalammar/intro-to-data-input-pipelines-with-tf-data)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "id": "lE6moeUeaBiG",
    "outputId": "86fbc3e3-0492-4e38-f6e6-379481ea4648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[3 4 5]\n",
      "[6 7 8]\n",
      "[ 9 10]\n"
     ]
    }
   ],
   "source": [
    "## BATCH\n",
    "\n",
    "number_items = 11\n",
    "number_list1 = np.arange(number_items)\n",
    "\n",
    "number_list1_dataset = tf.data.Dataset.from_tensor_slices(number_list1).batch(3, drop_remainder = False)\n",
    "\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(number_list1_dataset)\n",
    "\n",
    "for item in number_list1_dataset:\n",
    "    number = iterator.get_next().numpy()\n",
    "    print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BffrpfZ-aWG2"
   },
   "outputs": [],
   "source": [
    "## ZIP\n",
    "\n",
    "data_set1 = [1,2,3,4,5]\n",
    "data_set2 = ['a','e','i','o','u']\n",
    "data_set1 = tf.data.Dataset.from_tensor_slices(data_set1)\n",
    "data_set2 = tf.data.Dataset.from_tensor_slices(data_set2)\n",
    "\n",
    "zipped_datasets = tf.data.Dataset.zip((data_set1, data_set2))\n",
    "\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(zipped_datasets)\n",
    "\n",
    "for item in zipped_datasets:\n",
    "    number = iterator.get_next()\n",
    "    print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wsqo1MEbahyi"
   },
   "outputs": [],
   "source": [
    "## CONCAT\n",
    "\n",
    "datas1 = tf.data.Dataset.from_tensor_slices([1,2,3,5,7,11,13,17])\n",
    "datas2 = tf.data.Dataset.from_tensor_slices([19,23,29,31,37,41])\n",
    "\n",
    "datas3 = datas1.concatenate(datas2)\n",
    "\n",
    "print(datas3)\n",
    "\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(datas3)\n",
    "\n",
    "for i in range(14):\n",
    "    number = iterator.get_next()\n",
    "    print(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jcTAV330Sjgu"
   },
   "source": [
    "#### ___ImageDataGenerator___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "colab_type": "code",
    "id": "lRc4DQMG2XxD",
    "outputId": "0fff9bc9-31d8-4f91-8588-10b4a03a7bf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
      "228818944/228813984 [==============================] - 2s 0us/step\n",
      "Found 3670 images belonging to 5 classes.\n",
      "float32 (32, 256, 256, 3)\n",
      "float32 (32, 5)\n"
     ]
    }
   ],
   "source": [
    "flowers = tf.keras.utils.get_file(\n",
    "    'flower_photos',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    untar=True)\n",
    "\n",
    "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)\n",
    "\n",
    "images, labels = next(img_gen.flow_from_directory(flowers))\n",
    "\n",
    "print(images.dtype, images.shape)\n",
    "print(labels.dtype, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtVBBGNqbA3B"
   },
   "source": [
    "#### ___TFRecord___\n",
    "\n",
    "_TFRecord format is a binary file format. For large files, it is a good choice because binary files take up less disc space, take less time to copy, and can be read very efficiently from the disc. All this can have a significant effect on the efficiency of your data pipeline and thus, the training time of your model. The format is also optimized in a\n",
    "variety of ways for use with TensorFlow. It is a little complex because data has to be converted into\n",
    "the binary format prior to storage and decoded when read back._\n",
    "\n",
    "_A TFRecord file is a sequence of binary strings, its structure must be specified prior to saving so that it can be properly written and subsequently read back._\n",
    "\n",
    "_TensorFlow has two structures for this,_\n",
    "\n",
    "`tf.train.Example`\n",
    "\n",
    "`tf.train.SequenceExample. `\n",
    "\n",
    "_We have to store each sample of your data in one of these structures, then serialize it, and use_ `tf.python_io.TFRecordWriter` _to save it to disk._\n",
    "\n",
    "_A feature is a dictionary containing the data that is passed to tf.train.Example prior to serialization and saving the data._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ndWk2wUfjVQ"
   },
   "source": [
    "##### ___TF Record 1___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wCPatELo7v50"
   },
   "outputs": [],
   "source": [
    "# WRITE TFRECORD\n",
    "data = np.array([10.,11.,12.,13.,14.,15.])\n",
    "\n",
    "def npy_to_tfrecords(fname,data):\n",
    "    writer = tf.io.TFRecordWriter(fname)\n",
    "\n",
    "    feature={}\n",
    "    feature['data'] = tf.train.Feature(float_list=tf.train.FloatList(value=data))\n",
    "    \n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    \n",
    "    serialized = example.SerializeToString()\n",
    "    \n",
    "    writer.write(serialized)\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "npy_to_tfrecords(\"./myfile.tfrecords\",data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEAqCnsX7v54"
   },
   "source": [
    "_The code to read the record back is as follows._\n",
    "\n",
    "_A_ ___parse_function___ _function is constructed that decodes the dataset read back from the file. This requires a dictionary_ _(keys_to_features)_ _with the same name and structure as the saved data:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "lT3wWDxu7v55",
    "outputId": "5a324d14-1422-43e4-8feb-527560a2f2cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([10. 11. 12. 13. 14. 15.], shape=(6,), dtype=float32)\n",
      "[10. 11. 12. 13. 14. 15.]\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "data_set = tf.data.TFRecordDataset(\"./myfile.tfrecords\")\n",
    "\n",
    "def parse_function(example_proto):\n",
    "    keys_to_features = {'data':tf.io.FixedLenSequenceFeature([], dtype = tf.float32, allow_missing = True) }\n",
    "\n",
    "    parsed_features = tf.io.parse_single_example(serialized=example_proto, features=keys_to_features)\n",
    "\n",
    "    return parsed_features['data']\n",
    "\n",
    "data_set = data_set.map(parse_function)\n",
    "\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(data_set)\n",
    "\n",
    "# array is retrieved as one item\n",
    "item = iterator.get_next()\n",
    "print(item)\n",
    "\n",
    "print(item.numpy())\n",
    "\n",
    "print(item[2].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B-PWYZba7v58"
   },
   "source": [
    "##### ___TF Record  2___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lhWhFn797v59"
   },
   "outputs": [],
   "source": [
    "filename = './students.tfrecords'\n",
    "\n",
    "dataset = {\n",
    "'ID': 61553,\n",
    "'Name': ['Jones', 'Felicity'],\n",
    "'Scores': [45.6, 97.2]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1y9y2Ryy7v6A"
   },
   "source": [
    "_Using this, we can construct a tf.train.Example class, again using the `Feature()` method. Note how we have to encode our string:_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iMgT4una7v6A"
   },
   "outputs": [],
   "source": [
    "ID = tf.train.Feature(int64_list=tf.train.Int64List(value=[dataset['ID']]))\n",
    "Name = tf.train.Feature(bytes_list=tf.train.BytesList(value=[n.encode('utf-8') for n in dataset['Name']]))\n",
    "Scores = tf.train.Feature(float_list=tf.train.FloatList(value=dataset['Scores']))\n",
    "\n",
    "example = tf.train.Example(features=tf.train.Features(feature={'ID': ID, 'Name': Name, 'Scores': Scores }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hvg8EtWG7v6E"
   },
   "outputs": [],
   "source": [
    "writer_rec = tf.io.TFRecordWriter(filename)\n",
    "\n",
    "writer_rec.write(example.SerializeToString())\n",
    "\n",
    "writer_rec.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9mjgIfe7v6J"
   },
   "outputs": [],
   "source": [
    "data_set = tf.data.TFRecordDataset(\"./students.tfrecords\")\n",
    "\n",
    "def parse_function(example_proto):\n",
    "    keys_to_features = {'ID':tf.io.FixedLenFeature([], dtype = tf.int64),\n",
    "                        'Name':tf.io.VarLenFeature(dtype = tf.string),\n",
    "                        'Scores':tf.io.VarLenFeature(dtype = tf.float32)}\n",
    "\n",
    "    parsed_features = tf.io.parse_single_example(serialized=example_proto, features=keys_to_features)\n",
    "\n",
    "    return parsed_features[\"ID\"], parsed_features[\"Name\"],parsed_features[\"Scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u0YcS0fY7v6N"
   },
   "outputs": [],
   "source": [
    "dataset = data_set.map(parse_function)\n",
    "\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "items = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "5I_VDLDD7v6S",
    "outputId": "f287bbdc-4405-4623-cbb2-9c23c41b95b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=int64, numpy=61553>, <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7f9f902eda20>, <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7f9f902ed748>)\n"
     ]
    }
   ],
   "source": [
    "print(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qF1wNCWS7v6X"
   },
   "source": [
    "_Now we can extract our data from item (note that the string must be decoded (from bytes) where the default for our Python 3 is utf8). Note also that the string and the array of floats are returned as sparse arrays, and to extract them from the record, we use the sparse array value method:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "id": "z18vVcLQ7v6Y",
    "outputId": "3da4bf07-fec9-495c-81f7-7ffc6168c407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:  61553\n",
      "[b'Jones' b'Felicity']\n",
      "Name: Jones , Felicity\n",
      "Scores:  12.0\n"
     ]
    }
   ],
   "source": [
    "print(\"ID: \",items[0].numpy())\n",
    "\n",
    "name = items[1].values.numpy()\n",
    "print(name)\n",
    "name1 = name[0].decode()\n",
    "name2 = name[1].decode('utf8')\n",
    "\n",
    "print(\"Name:\", name1,\",\",name2)\n",
    "\n",
    "print(\"Scores: \",item[2].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rroqsJB57v5e"
   },
   "source": [
    "#### ___Comma-Separated Values (CSV)___\n",
    "\n",
    "_CSV files are a very popular method of storing data. TensorFlow 2 contains flexible methods for dealing with them._\n",
    "\n",
    "`tf.data.experimental.CsvDataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fX9p2bb7v5f"
   },
   "source": [
    "##### ___CSV 1___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LL_lTrJR7v5g"
   },
   "source": [
    "_With the following arguments, our dataset will consist of two items taken from each row of the filename file, both of the float type, with the first line of the file ignored and columns 1 and 2 used (column numbering is, of course, 0-based):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0aNaQRZ57v5h"
   },
   "outputs": [],
   "source": [
    "filename = [\"./size_1000.csv\"]\n",
    "record_defaults = [tf.float32] * 2 # two required float columns\n",
    "\n",
    "data_set = tf.data.experimental.CsvDataset(filename, record_defaults, header=True, select_cols=[1,2])\n",
    "\n",
    "for item in data_set:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WnioipcQ7v5k"
   },
   "source": [
    "##### ___CSV 2___\n",
    "\n",
    "_In this example, and with the following arguments, our dataset will consist of one required float, one optional float with a default value of 0.0, and an int, where there is no header in the CSV file and only columns 1, 2, and 3 are imported:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "spye_KGV7v5q",
    "outputId": "c839d996-a8a1-4665-ad6b-09568e1df3bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=float32, numpy=428000.0>, <tf.Tensor: shape=(), dtype=float32, numpy=555.0>, <tf.Tensor: shape=(), dtype=int32, numpy=42>)\n",
      "(<tf.Tensor: shape=(), dtype=float32, numpy=-5.3>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: shape=(), dtype=int32, numpy=69>)\n"
     ]
    }
   ],
   "source": [
    "filename = \"mycsvfile.txt\"\n",
    "record_defaults = [tf.float32, tf.constant([0.0], dtype=tf.float32), tf.int32,]\n",
    "\n",
    "data_set = tf.data.experimental.CsvDataset(filename, record_defaults, header=False, select_cols=[1,2,3])\n",
    "\n",
    "for item in data_set:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mAf29wt17v5u"
   },
   "source": [
    "##### ___CSV 3___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FOr2ydda7v5v",
    "outputId": "0dd5e491-91ab-4d3c-c551-221afe37ab6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.6 23.4  Abc.co.uk\n",
      "98.7 56.8  Xyz.com\n",
      "34.2 68.1  Pqr.net\n"
     ]
    }
   ],
   "source": [
    "filename = \"file1.txt\"\n",
    "record_defaults = [tf.float32, tf.float32, tf.string ,]\n",
    "\n",
    "dataset = tf.data.experimental.CsvDataset(filename, record_defaults, header=False)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item[0].numpy(), item[1].numpy(),item[2].numpy().decode() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wRQy8CVqZlDF"
   },
   "source": [
    "#### ___Dataset in Action___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "ma5h_ka37v3K",
    "outputId": "af239d37-473f-41ac-f2c5-faf089de080a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(train_x,train_y), (test_x, test_y) = mnist.load_data()\n",
    "train_x, test_x = train_x/255.0, test_x/255.0\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gdjG1L4f7v3T"
   },
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(32).shuffle(10000)\n",
    "training_dataset = training_dataset.map(lambda x, y: (tf.image.random_flip_left_right(x), y))\n",
    "training_dataset = training_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yhfl66IA7v3Z"
   },
   "outputs": [],
   "source": [
    "testing_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(32).shuffle(10000)\n",
    "testing_dataset = training_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9x1qczzW7v3n"
   },
   "outputs": [],
   "source": [
    "# Now in the fit() function, we can pass the dataset directly in, as follows:\n",
    "\n",
    "model5 = tf.keras.models.Sequential([\n",
    " tf.keras.layers.Flatten(),\n",
    " tf.keras.layers.Dense(512,activation=tf.nn.relu),\n",
    " tf.keras.layers.Dropout(0.2),\n",
    " tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ISER9WS7v3v"
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_x)//32 # required becuase of the repeat() on the dataset\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam()\n",
    "\n",
    "model5.compile (optimizer= optimiser, loss='sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "zOOCdKsu7v33",
    "outputId": "5360910f-30b3-4f7a-db0a-7ee935948235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Layer flatten is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3612 - accuracy: 0.8900\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1827 - accuracy: 0.9442\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1400 - accuracy: 0.9568\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1165 - accuracy: 0.9645\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1000 - accuracy: 0.9693\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0912 - accuracy: 0.9714\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0823 - accuracy: 0.9737\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0748 - accuracy: 0.9766\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0686 - accuracy: 0.9782\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0644 - accuracy: 0.9793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9f872e89e8>"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.fit(training_dataset, epochs=epochs, steps_per_epoch = steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "9b0IdB5H7v4A",
    "outputId": "ffc8ab42-6943-4068-b956-dbe9c7f56b5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 0.9937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.01874697580933571, 0.9937499761581421]"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.evaluate(testing_dataset,steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tw4AP3E87v4J"
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "callbacks = [\n",
    "  # Write TensorBoard logs to `./logs` directory\n",
    "  tf.keras.callbacks.TensorBoard(log_dir='log/{}/'.format(dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "colab_type": "code",
    "id": "nq4VK25C7v4P",
    "outputId": "554d06b3-8a37-4f1c-8db8-900565c0550e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0607 - accuracy: 0.9806 - val_loss: 0.0261 - val_accuracy: 0.9896\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0575 - accuracy: 0.9817 - val_loss: 0.0467 - val_accuracy: 0.9688\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0525 - accuracy: 0.9826 - val_loss: 0.0614 - val_accuracy: 0.9688\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0496 - accuracy: 0.9840 - val_loss: 0.0659 - val_accuracy: 0.9792\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0474 - accuracy: 0.9846 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0477 - accuracy: 0.9845 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0457 - accuracy: 0.9848 - val_loss: 0.0244 - val_accuracy: 0.9896\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0423 - accuracy: 0.9862 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0406 - accuracy: 0.9863 - val_loss: 0.0273 - val_accuracy: 0.9896\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0391 - accuracy: 0.9874 - val_loss: 0.0457 - val_accuracy: 0.9896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9f872e8a90>"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.fit(training_dataset, epochs = epochs, steps_per_epoch = steps_per_epoch,\n",
    "          validation_data = testing_dataset,\n",
    "          validation_steps = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "5UOl_S5z7v4Y",
    "outputId": "605f572b-df79-4a00-cbb3-2ec62fc00e59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0296 - accuracy: 0.9937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.02959929034113884, 0.9937499761581421]"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.evaluate(testing_dataset,steps=10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "7. Keras Overview.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
